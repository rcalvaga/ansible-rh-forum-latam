################################################################################################
# Creating the Whole RHT Demo Forum LATAM 2016 Environment in AWS                              #
#                                                                                              #
# By Robert J. Calva - Red Hat LATAM - 2016                                                    #
#                                                                                              #
# NOTE: just set the aws_region, aws_zone, name_zone and volume_type variables as you wish!    #
#                                                                                              #
# IMPORTANT: regarding storage_type information, please take a look at this link:              #
# http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html                       #
#                                                                                              #
# Variable examples:                                                                           #
#                                                                                              #
# aws_region could be: sa-east-1 or us-east-1                                                  #
# aws_zone could be: sa-east-1a, sa-east-1b, us-east-1a, us-east-1b                            #
# name_zone MUST be: latam.forum.redhat.com                                                    #
# volume_type could be: standard or gp2                                                        #
#                                                                                              #
# Enjoy!                                                                                       #
################################################################################################
- name: Create The RHT Demo Forum LATAM 2016 Environment in AWS
  hosts: localhost
  gather_facts: false
  connection: local
  vars:
        - aws_region: sa-east-1
        - aws_zone: sa-east-1a
        - name_zone: latam.forum.redhat.com
        - volume_type: gp2
  tasks:
  - name: Create the VPC in AWS {{ aws_region }}
    ec2_vpc:
        state: present
        cidr_block: 192.168.0.0/16
        resource_tags: { "Name":"RHT Demo Forum LATAM 2016 - {{ aws_region }}" }
        subnets:
          - cidr: 192.168.1.0/24
            az: "{{ aws_zone }}"
            resource_tags: { "Name":"rht-forum-latam-private-subnet" }
        internet_gateway: True
        route_tables:
          - subnets:
              - 192.168.1.0/24
            routes:
              - dest: 0.0.0.0/0
                gw: igw
        region: "{{ aws_region }}"
        wait: yes
    register: vpc

  - set_fact: vpc_id={{ vpc.vpc_id }}
  - set_fact: subnet_id={{ vpc.subnets[0].id }}

  - name: Create an AWS Keypair
    ec2_key: name="rht-forum-demo-2016-key" region={{ aws_region }}
    register: rht_forum_demo_key

  - set_fact: private_key={{ rht_forum_demo_key.key.private_key }}
    when: rht_forum_demo_key.key.private_key is defined

  - name: Write the new AWS Keypair into a file... /tmp/rht-forum-demo-2016-key.pem
    copy: content={{ private_key }} dest="/tmp/rht-forum-demo-2016-key.pem" mode=0600
    when: rht_forum_demo_key.changed

  - name: Create Locust Security Group
    ec2_group:
      name: "locust-sec-group"
      region: "{{ aws_region }}"
      description: Locust Security Group
      vpc_id: "{{ vpc_id }}"
      rules:
        - proto: tcp
          from_port: 8089
          to_port: 8089
          cidr_ip: 0.0.0.0/0
        - proto: tcp
          from_port: 8090
          to_port: 8090
          cidr_ip: 0.0.0.0/0
        - proto: tcp
          from_port: 22
          to_port: 22
          cidr_ip: 0.0.0.0/0
    register: locust-sec-group

  - name: Create OSP Master Security Group
    ec2_group:
      name: "openshift-master-sec-group"
      region: "{{ aws_region }}"
      description: OSP Master Security Group
      vpc_id: "{{ vpc_id }}"
      rules:
        - proto: tcp
          from_port: 22
          to_port: 22
          cidr_ip: 0.0.0.0/0
        - proto: tcp
          from_port: 8443
          to_port: 8443
          cidr_ip: 0.0.0.0/0
        - proto: all
          from_port: all
          to_port: all
          cidr_ip: 192.168.0.0/16
    register: openshift-master-sec-group

  - name: Create OSP Infra Security Group
    ec2_group:
      name: "openshift-infra-sec-group"
      region: "{{ aws_region }}"
      description: OSP Infra Security Group
      vpc_id: "{{ vpc_id }}"
      rules:
        - proto: tcp
          from_port: 80
          to_port: 80
          cidr_ip: 0.0.0.0/0
        - proto: tcp
          from_port: 443
          to_port: 443
          cidr_ip: 0.0.0.0/0
        - proto: tcp
          from_port: 1936
          to_port: 1936
          cidr_ip: 0.0.0.0/0
        - proto: all
          from_port: all
          to_port: all
          cidr_ip: 192.168.0.0/16
    register: openshift-infra-sec-group

  - name: Create OSP Nodes Security Group
    ec2_group:
      name: "openshift-nodes-sec-group"
      region: "{{ aws_region }}"
      description: OSP Nodes Security Group
      vpc_id: "{{ vpc_id }}"
      rules:
        - proto: all
          from_port: all
          to_port: all
          cidr_ip: 192.168.0.0/16
    register: openshift-nodes-sec-group

  - name: Create Ansible Security Group
    ec2_group:
      name: "ansible-sec-group"
      region: "{{ aws_region }}"
      description: Ansible Security Group
      vpc_id: "{{ vpc_id }}"
      rules:
        - proto: tcp
          from_port: 80
          to_port: 80
          cidr_ip: 0.0.0.0/0
        - proto: tcp
          from_port: 443
          to_port: 443
          cidr_ip: 0.0.0.0/0
        - proto: all
          from_port: all
          to_port: all
          cidr_ip: 192.168.0.0/16
    register: ansible-sec-group

  - name: Create CloudForms Security Group
    ec2_group:
      name: "cloudforms-sec-group"
      region: "{{ aws_region }}"
      description: CloudForms Security Group
      vpc_id: "{{ vpc_id }}"
      rules:
        - proto: tcp
          from_port: 80
          to_port: 80
          cidr_ip: 0.0.0.0/0
        - proto: tcp
          from_port: 443
          to_port: 443
          cidr_ip: 0.0.0.0/0
        - proto: all
          from_port: all
          to_port: all
          cidr_ip: 192.168.0.0/16
    register: cloudforms-sec-group

  - name: Create Route53 RHT Forum Demo 2016 Private DNS Zone
    route53_zone: zone={{ name_zone }} state=present vpc_region={{ aws_region }} vpc_id={{ vpc_id }} comment='RHT Forum LATAM 2016 DNS Private Zone'
    register: rht_forum_private_zone

  - name: Create Route53 DNS Record For Ansible Tower into {{name_zone}}
    route53:
      command: create
      zone: "{{ name_zone }}"
      private_zone: true
      vpc_id: "{{ vpc_id }}"
      record: ansible3.{{ name_zone }}
      type: A
      ttl: 300
      value: 192.168.1.21
      wait: yes

  - name: Create Route53 DNS Record For CloudForms1 into {{name_zone}}
    route53:
      command: create
      zone: "{{ name_zone }}"
      private_zone: true
      vpc_id: "{{ vpc_id }}"
      record: cloudforms1.{{ name_zone }}
      type: A
      ttl: 300
      value: 192.168.1.10
      wait: yes

  - name: Create Route53 DNS Record For CloudForms2 into {{name_zone}}
    route53:
      command: create
      zone: "{{ name_zone }}"
      private_zone: true
      vpc_id: "{{ vpc_id }}"
      record: cloudforms2.{{ name_zone }}
      type: A
      ttl: 300
      value: 192.168.1.11
      wait: yes

  - name: Create Route53 DNS Record For CloudForms3 into {{name_zone}}
    route53:
      command: create
      zone: "{{ name_zone }}"
      private_zone: true
      vpc_id: "{{ vpc_id }}"
      record: cloudforms3.{{ name_zone }}
      type: A
      ttl: 300
      value: 192.168.1.12
      wait: yes

  - name: Create Route53 DNS Record For OSE Infra into {{name_zone}}
    route53:
      command: create
      zone: "{{ name_zone }}"
      private_zone: true
      vpc_id: "{{ vpc_id }}"
      record: infra.{{ name_zone }}
      type: A
      ttl: 300
      value: 192.168.1.101  
      wait: yes

  - name: Create Route53 DNS Record For OSE Master into {{name_zone}}
    route53:
      command: create
      zone: "{{ name_zone }}"
      private_zone: true
      vpc_id: "{{ vpc_id }}"
      record: master.{{ name_zone }}
      type: A
      ttl: 300
      value: 192.168.1.100
      wait: yes

  - name: Create Route53 DNS Record For OSE Node1 into {{name_zone}}
    route53:
      command: create
      zone: "{{ name_zone }}"
      private_zone: true
      vpc_id: "{{ vpc_id }}"
      record: node1.{{ name_zone }}
      type: A
      ttl: 300
      value: 192.168.1.102
      wait: yes

  - name: Create Route53 DNS Record For OSE Node2 into {{name_zone}}
    route53:
      command: create
      zone: "{{ name_zone }}"
      private_zone: true
      vpc_id: "{{ vpc_id }}"
      record: node2.{{ name_zone }}
      type: A
      ttl: 300
      value: 192.168.1.103
      wait: yes

  - name: Create Route53 DNS Record For Locust into {{name_zone}}
    route53:
      command: create
      zone: "{{ name_zone }}"
      private_zone: true
      vpc_id: "{{ vpc_id }}"
      record: locust.{{ name_zone }}
      type: A
      ttl: 300
      value: 192.168.1.30
      wait: yes

  - name: Creating Locust Instance
    ec2:
      key_name: rht-forum-demo-2016-key
      group: locust-sec-group
      instance_type: t2.medium
      image: ami-459a0729
      wait: yes
      wait_timeout: 500
      volumes:
      - device_name: /dev/sda1
        volume_type: "{{ volume_type }}"
        volume_size: 10
        delete_on_termination: true
      instance_initiated_shutdown_behavior: stop
      private_ip: 192.168.1.30
      termination_protection: no
      instance_tags:
        Name: Locust
      exact_count: 1
      count_tag:
        Name: Locust
      region: "{{ aws_region }}"
      vpc_subnet_id: "{{ subnet_id }}"
      zone: "{{ aws_zone }}"
      assign_public_ip: yes
    register: locust
#  - name: Wait for Locust Instance to come up
#    wait_for: host={{ item.public_dns_name }} port=8089 delay=60 timeout=320 state=started
#    with_items: '{{locust.instances}}'

  - name: Creating OSE Master Instance
    ec2:
      key_name: rht-forum-demo-2016-key
      group: openshift-master-sec-group
      instance_type: c3.2xlarge
      image: ami-31c15c5d
      wait: yes
      wait_timeout: 500
      volumes:
      - device_name: /dev/sda1
        volume_type: "{{ volume_type }}"
        volume_size: 100
        delete_on_termination: true
      - device_name: /dev/sdb
        volume_type: "{{ volume_type }}"
        volume_size: 500
        delete_on_termination: true
      instance_initiated_shutdown_behavior: stop
      private_ip: 192.168.1.100
      termination_protection: no
      instance_tags:
        Name: openshift-master
      exact_count: 1
      count_tag:
        Name: openshift-master
      region: "{{ aws_region }}"
      vpc_subnet_id: "{{ subnet_id }}"
      zone: "{{ aws_zone }}"
      assign_public_ip: yes
    register: openshift_master
#  - name: Wait for OSE Master Instance to come up
#    wait_for: host={{ item.public_dns_name }} port=8443 delay=60 timeout=320 state=started
#    with_items: '{{openshift_master.instances}}'

  - name: Creating OSE Infra Instance
    ec2:
      key_name: rht-forum-demo-2016-key
      group: openshift-infra-sec-group
      instance_type: c3.2xlarge
      image: ami-33fc615f
      wait: yes
      wait_timeout: 500
      volumes:
      - device_name: /dev/sda1
        volume_type: "{{ volume_type }}"
        volume_size: 100
        delete_on_termination: true
      - device_name: /dev/sdb
        volume_type: "{{ volume_type }}"
        volume_size: 150
        delete_on_termination: true
      instance_initiated_shutdown_behavior: stop
      private_ip: 192.168.1.101
      termination_protection: no
      instance_tags:
        Name: openshift-infra
      exact_count: 1
      count_tag:
        Name: openshift-infra
      region: "{{ aws_region }}"
      vpc_subnet_id: "{{ subnet_id }}"
      zone: "{{ aws_zone }}"
      assign_public_ip: yes
    register: openshift_infra
#  - name: Wait for OSE Infra Instance to come up
#    wait_for: host={{ item.public_dns_name }} port=443 delay=60 timeout=320 state=started
#    with_items: '{{openshift_infra.instances}}'

  - name: Creating OSE Node1 Instance
    ec2:
      key_name: rht-forum-demo-2016-key
      group: openshift-nodes-sec-group
      instance_type: c3.2xlarge
      image: ami-99c25ff5
      wait: yes
      wait_timeout: 500
      volumes:
      - device_name: /dev/sda1
        volume_type: "{{ volume_type }}"
        volume_size: 100
        delete_on_termination: true
      - device_name: /dev/sdb
        volume_type: "{{ volume_type }}"
        volume_size: 150
        delete_on_termination: true
      instance_initiated_shutdown_behavior: stop
      private_ip: 192.168.1.102
      termination_protection: no
      instance_tags:
        Name: openshift-node1
      exact_count: 1
      count_tag:
        Name: openshift-node1
      region: "{{ aws_region }}"
      vpc_subnet_id: "{{ subnet_id }}"
      zone: "{{ aws_zone }}"
      assign_public_ip: yes
    register: openshift_node1

  - name: Creating OSE Node2 Instance
    ec2:
      key_name: rht-forum-demo-2016-key
      group: openshift-nodes-sec-group
      instance_type: c3.2xlarge
      image: ami-51fa673d
      wait: yes
      wait_timeout: 500
      volumes:
      - device_name: /dev/sda1
        volume_type: "{{ volume_type }}"
        volume_size: 100
        delete_on_termination: true
      - device_name: /dev/sdb
        volume_type: "{{ volume_type }}"
        volume_size: 150
        delete_on_termination: true
      instance_initiated_shutdown_behavior: stop
      private_ip: 192.168.1.103
      termination_protection: no
      instance_tags:
        Name: openshift-node2
      exact_count: 1
      count_tag:
        Name: openshift-node2
      region: "{{ aws_region }}"
      vpc_subnet_id: "{{ subnet_id }}"
      zone: "{{ aws_zone }}"
      assign_public_ip: yes
    register: openshift_node2

  - name: Creating Ansible Instance
    ec2:
      key_name: rht-forum-demo-2016-key
      group: ansible-sec-group
      instance_type: c4.xlarge
      image: ami-5068fa3c
      wait: yes
      wait_timeout: 500
      volumes:
      - device_name: /dev/sda1
        volume_type: "{{ volume_type }}"
        volume_size: 40
        delete_on_termination: true
      instance_initiated_shutdown_behavior: stop
      private_ip: 192.168.1.21
      termination_protection: no
      instance_tags:
        Name: Ansible-Tower
      exact_count: 1
      count_tag:
        Name: Ansible-Tower
      region: "{{ aws_region }}"
      vpc_subnet_id: "{{ subnet_id }}"
      zone: "{{ aws_zone }}"
      assign_public_ip: yes
    register: ansible_tower
#  - name: Wait for Ansible Tower Instance to come up
#    wait_for: host={{ item.public_dns_name }} port=443 delay=60 timeout=320 state=started
#    with_items: '{{ansible_tower.instances}}'

  - name: Creating CloudForms Worker1 Instance
    ec2:
      key_name: rht-forum-demo-2016-key
      group: cloudforms-sec-group
      instance_type: c4.xlarge
      image: ami-3bfa6757
      wait: yes
      wait_timeout: 500
      volumes:
      - device_name: /dev/sda1
        volume_type: "{{ volume_type }}"
        volume_size: 40
        delete_on_termination: true
      instance_initiated_shutdown_behavior: stop
      private_ip: 192.168.1.11
      termination_protection: no
      instance_tags:
        Name: CF-Worker1
      exact_count: 1
      count_tag:
        Name: CF-Worker1
      region: "{{ aws_region }}"
      vpc_subnet_id: "{{ subnet_id }}"
      zone: "{{ aws_zone }}"
      assign_public_ip: yes
    register: cf_worker1
#  - name: Wait for CloudForms Worker1 Instance to come up
#    wait_for: host={{ item.public_dns_name }} port=443 delay=60 timeout=320 state=started
#    with_items: '{{cf_worker1.instances}}'

  - name: Creating CloudForms Worker1 Instance
    ec2:
      key_name: rht-forum-demo-2016-key
      group: cloudforms-sec-group
      instance_type: c4.xlarge
      image: ami-51f4693d
      wait: yes
      wait_timeout: 500
      volumes:
      - device_name: /dev/sda1
        volume_type: "{{ volume_type }}"
        volume_size: 40
        delete_on_termination: true
      instance_initiated_shutdown_behavior: stop
      private_ip: 192.168.1.12
      termination_protection: no
      instance_tags:
        Name: CF-Worker2
      exact_count: 1
      count_tag:
        Name: CF-Worker2
      region: "{{ aws_region }}"
      vpc_subnet_id: "{{ subnet_id }}"
      zone: "{{ aws_zone }}"
      assign_public_ip: yes
    register: cf_worker2
#  - name: Wait for CloudForms Worker2 Instance to come up
#    wait_for: host={{ item.public_dns_name }} port=443 delay=60 timeout=320 state=started
#    with_items: '{{cf_worker2.instances}}'

  - name: Creating CloudForms VMDB Instance
    ec2:
      key_name: rht-forum-demo-2016-key
      group: cloudforms-sec-group
      instance_type: c4.xlarge
      image: ami-d7c05dbb
      wait: yes
      wait_timeout: 500
      volumes:
      - device_name: /dev/sda1
        volume_type: "{{ volume_type }}"
        volume_size: 45
        delete_on_termination: true
      - device_name: /dev/sdb
        volume_type: "{{ volume_type }}"
        volume_size: 20
        delete_on_termination: true
      instance_initiated_shutdown_behavior: stop
      private_ip: 192.168.1.10
      termination_protection: no
      instance_tags:
        Name: CF-VMDB
      exact_count: 1
      count_tag:
        Name: CF-VMDB
      region: "{{ aws_region }}"
      vpc_subnet_id: "{{ subnet_id }}"
      zone: "{{ aws_zone }}"
      assign_public_ip: yes
    register: cf_vmdb
